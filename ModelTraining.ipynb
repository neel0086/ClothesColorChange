{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **DATA UTILS**","metadata":{"id":"WQpsHuGZiOP7"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nimport math\nimport os\n\n# plot history for accuracy\ndef plot_acc(history, title = None):\n    fig = plt.figure()\n    if not isinstance(history, dict):\n        history = history.history\n\n    plt.plot(history['acc'])\n    plt.plot(history['val_acc'])\n    if title is not None:\n        plt.title(title)\n\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc=0)\n\n# plot history for loss\ndef plot_loss(history, title = None):\n    fig = plt.figure()\n    if not isinstance(history, dict):\n        history = history.history\n\n    plt.plot(history['loss'])\n    plt.plot(history['val_loss'])\n    if title is not None:\n        plt.title(title)\n\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n\n    plt.legend(['Train', 'Val'], loc=0)\n\n# show results using each model\ndef show_result(model, x, y, title = 'Result', save_file = False, save_file_name = 'show_result', IMG_HEIGHT = 384, IMG_WIDTH = 256):\n    if len(x) != len(y):\n        raise Exception('Check the number of images and labels')\n    else:\n        pass\n\n    x = x.reshape([1, IMG_HEIGHT, IMG_WIDTH, 3])\n    pred = model.predict(x)\n    pred = np.argmax(pred, axis = 3)\n\n    y = y.reshape([1, IMG_HEIGHT, IMG_WIDTH, 4])\n    gt = np.argmax(y, axis = 3)\n\n    fig = plt.figure(figsize = (12, 5))\n    plt.suptitle(title, fontsize=25)\n    plt.subplot(1,4,1)\n    plt.imshow(np.squeeze(x))\n    plt.title('Image')\n    plt.subplot(1,4,2)\n    plt.imshow(np.squeeze(gt))\n    plt.title('Ground truth')\n    plt.subplot(1,4,3)\n    plt.imshow(np.squeeze(pred))\n    plt.title('Prediction')\n    plt.subplot(1,4,4)\n    plt.imshow(np.squeeze(x))\n    masked_imclass = np.ma.masked_where(np.squeeze(pred) == 0, np.squeeze(pred))\n    plt.imshow( masked_imclass, alpha = 0.4)\n    plt.title('Overlay')\n    plt.show()\n\n    # save results\n    if save_file == True:\n        save_dir = './result/show_result/'\n        if not os.path.exists(save_dir): # if there is no exist, make the path\n            os.makedirs(save_dir)\n        fig.savefig(save_dir + save_file_name)\n    return\n\n# save image segmented by skin, hair, clothes\ndef iamge_segmentation(model, x, title = 'Each Segmentation',save_file = False, save_file_name = 'iamge_segmentation', IMG_HEIGHT = 384, IMG_WIDTH = 256):\n    x = x.reshape([1, IMG_HEIGHT, IMG_WIDTH, 3])\n\n    pred = model.predict(x)\n    pred = np.argmax(pred, axis = 3)\n    empty = np.zeros((pred.shape))\n\n    fig = plt.figure(figsize = (12, 5))\n    plt.subplot(1,4,1)\n    plt.imshow(np.squeeze(x))\n    plt.title('Image')\n    plt.subplot(1,4,2)\n    plt.imshow(np.squeeze(x))\n    masked_imclass = np.ma.masked_where(np.squeeze(pred) == 1, np.squeeze(empty))\n    plt.imshow( masked_imclass, alpha = 1)\n    plt.title('Skin')\n    plt.subplot(1,4,3)\n    plt.imshow(np.squeeze(x))\n    masked_imclass = np.ma.masked_where(np.squeeze(pred) == 2, np.squeeze(empty))\n    plt.imshow( masked_imclass, alpha = 1)\n    plt.title('Hair')\n    plt.subplot(1,4,4)\n    plt.imshow(np.squeeze(x))\n    masked_imclass = np.ma.masked_where(np.squeeze(pred) == 3, np.squeeze(empty))\n    plt.imshow( masked_imclass, alpha = 1)\n    plt.title('Clothes')\n    plt.show()\n    if save_file == True:\n        save_dir = './result/iamge_segmentation/'\n        if not os.path.exists(save_dir): # if there is no exist, make the path\n            os.makedirs(save_dir)\n        fig.savefig(save_dir + save_file_name)\n\n    return","metadata":{"id":"YzGu510liTk8","execution":{"iopub.status.busy":"2023-09-28T18:30:11.883377Z","iopub.execute_input":"2023-09-28T18:30:11.883755Z","iopub.status.idle":"2023-09-28T18:30:20.218935Z","shell.execute_reply.started":"2023-09-28T18:30:11.883704Z","shell.execute_reply":"2023-09-28T18:30:20.217905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **MODEL AND FUNCTIONS**\n\n","metadata":{"id":"SKS_vZi6hxzr"}},{"cell_type":"code","source":"import keras\nfrom keras import models\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D, Input, Concatenate, Conv2DTranspose\nfrom keras.optimizers import Adam\nfrom keras.layers import BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n\nimport os\n\nclass UNet(object):\n    def __init__(self, img_shape, num_of_class, actf = 'relu',\n        learning_rate = 0.001,  drop_rate = 0.5, do_batch_norm = False, do_drop = False):\n        self.learning_rate = learning_rate\n        self.actf = actf\n        self.img_shape = img_shape\n        self.num_of_class = num_of_class\n        self.drop_rate = drop_rate\n        self.do_batch_norm = do_batch_norm\n        self.do_drop = do_drop\n\n        self.model = self.build_model()\n\n    # encoding block(conv - conv - pool)\n    def enc_conv_block(self, inputs, feature_maps, filter_size = (3, 3),\n                           conv_strides = 1, pooling_filter_size = (2, 2), pooling_strides = (2, 2)):\n        conv1 = Conv2D(feature_maps , filter_size , activation = self.actf, strides = conv_strides,\n                           padding = 'same', kernel_initializer = 'he_normal')(inputs)\n        conv2 = Conv2D(feature_maps , filter_size , activation = self.actf, strides = conv_strides,\n                           padding = 'same', kernel_initializer = 'he_normal')(conv1)\n        pool = MaxPooling2D(pooling_filter_size, strides = pooling_strides)(conv2)\n\n        return pool, conv2\n\n    # decoding block(concat - upconv - upconv)\n    def dec_conv_block(self, inputs, merge_inputs, feature_maps, filter_size = (3, 3), conv_strides = 1,\n                           up_conv_strides = (2, 2)):\n\n        merge = Concatenate(axis = 3)([Conv2DTranspose(feature_maps, filter_size,\n                                                       activation = self.actf, strides = up_conv_strides, kernel_initializer = 'he_normal',\n                                                       padding = 'same')(inputs), merge_inputs])\n\n        conv1 = Conv2D(feature_maps , filter_size , activation = self.actf, strides = conv_strides,\n                           padding = 'same', kernel_initializer = 'he_normal')(merge)\n        conv2 = Conv2D(feature_maps , filter_size , activation = self.actf, strides = conv_strides,\n                           padding = 'same', kernel_initializer = 'he_normal')(conv1)\n\n        return conv2\n\n    # encoder\n    def encoding_path(self, inputs):\n\n        enc_conv1, concat1 = self.enc_conv_block(inputs, 64)\n        enc_conv2, concat2 = self.enc_conv_block(enc_conv1, 128)\n        enc_conv3, concat3 = self.enc_conv_block(enc_conv2, 256)\n        enc_conv4, concat4 = self.enc_conv_block(enc_conv3, 512)\n\n        return concat1, concat2, concat3, concat4, enc_conv4\n\n    # decoder\n    def decoding_path(self, dec_inputs, concat1, concat2, concat3, concat4):\n\n        dec_conv1 = self.dec_conv_block(dec_inputs, concat4, 512)\n        dec_conv2 = self.dec_conv_block(dec_conv1, concat3, 256)\n        dec_conv3 = self.dec_conv_block(dec_conv2, concat2, 128)\n        dec_conv4 = self.dec_conv_block(dec_conv3, concat1, 64)\n\n        return dec_conv4\n    # build network\n    def build_model(self):\n        inputs = Input(self.img_shape)\n\n        # Contracting path\n        concat1, concat2, concat3, concat4, enc_path = self.encoding_path(inputs)\n\n        # middle path\n        mid_path1 = Conv2D(1024, (3,3), activation = self.actf, padding = 'same', kernel_initializer = 'he_normal')(enc_path)\n        mid_path1 = Dropout(self.drop_rate)(mid_path1)\n        mid_path2 = Conv2D(1024, (3,3), activation = self.actf, padding = 'same', kernel_initializer = 'he_normal')(mid_path1)\n        mid_path2 = Dropout(self.drop_rate)(mid_path2)\n\n        # Expanding path\n        dec_path = self.decoding_path(mid_path2, concat1, concat2, concat3, concat4)\n        segmented = Conv2D(self.num_of_class, (1,1), activation = self.actf, padding = 'same', kernel_initializer = 'he_normal')(dec_path)\n        segmented = Activation('softmax')(segmented)\n\n        model = Model(inputs = inputs, outputs = segmented)\n        model.compile(optimizer = Adam(lr = self.learning_rate),\n                          loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\n        return model\n\n    # train model\n    def train(self, X_train, Y_train, epoch = 10, batch_size = 32, val_split = 0.2, shuffle = True):\n\n        self.history = self.model.fit(X_train, Y_train, validation_split = val_split,\n                                          epochs = epoch, batch_size = batch_size, shuffle =  shuffle)\n        return self.history\n\n    # train with data augmentation\n    def train_generator(self, x_train, y_train, x_test, y_test, name_model, epoch = 10, batch_size = 32, val_split = 0.2, min_lr = 1e-06):\n\n        train_datagen = ImageDataGenerator(\n            rescale=1./255,\n            brightness_range=[0.7, 1.3]\n        )\n\n        val_datagen = ImageDataGenerator(\n            rescale=1./255\n        )\n\n        train_gen = train_datagen.flow(\n            x_train,\n            y_train,\n            batch_size = batch_size,\n            shuffle=True\n        )\n\n        val_gen = val_datagen.flow(\n            x_test,\n            y_test,\n            batch_size = batch_size,\n            shuffle=False\n        )\n\n        save_dir = './save_model/'\n        if not os.path.exists(save_dir): # if there is no exist, make the path\n            os.makedirs(save_dir)\n\n        cb_checkpoint = ModelCheckpoint(save_dir + name_model + '.h5', monitor = 'val_acc', save_best_only = True, verbose = 1)\n        reduce_lr = ReduceLROnPlateau(monitor = 'val_acc', factor = 0.2, patience = 5, verbose = 1, min_lr = min_lr)\n\n        self.history = self.model.fit_generator(train_gen,\n                                                validation_data=val_gen,\n                                                epochs=epoch,\n                                                callbacks=[cb_checkpoint, reduce_lr])\n        return self.history\n    # predict test data\n    def predict(self, X_test):\n        pred_classes = self.model.predict(X_test)\n\n        return pred_classes\n\n    # show architecture\n    def show_model(self):\n        return print(self.model.summary())\n\n    # reuse model\n    def saved_model_use(self, save_dir = None):\n        if save_dir == None:\n            return print('No path')\n\n        self.model.load_weights(save_dir)\n\n        return print(\"Loaded model from '{}'\".format(save_dir))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T18:38:42.333225Z","iopub.execute_input":"2023-09-28T18:38:42.333609Z","iopub.status.idle":"2023-09-28T18:38:42.368648Z","shell.execute_reply.started":"2023-09-28T18:38:42.333577Z","shell.execute_reply":"2023-09-28T18:38:42.367509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nIMG_HEIGHT = 384\nIMG_WIDTH = 256\nBATCH_SIZE = 8\n\nx_train = np.load('../input/clothesdata/datasets/x_train.npy')\nx_test = np.load('../input/clothesdata/datasets/x_test.npy')\ny_train = np.load('../input/clothesdata/datasets/y_train_onehot.npy')\ny_test = np.load('../input/clothesdata/datasets/y_test_onehot.npy')\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)","metadata":{"id":"9oHJqDIYjJqy","outputId":"86165897-ebe9-4503-a822-35963250d274","execution":{"iopub.status.busy":"2023-09-28T18:30:20.453411Z","iopub.execute_input":"2023-09-28T18:30:20.453712Z","iopub.status.idle":"2023-09-28T18:30:21.110927Z","shell.execute_reply.started":"2023-09-28T18:30:20.453684Z","shell.execute_reply":"2023-09-28T18:30:21.110020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = UNet(img_shape = x_train[0].shape, num_of_class = 4,learning_rate = 1e-3)","metadata":{"id":"kulPi2eUjPO2","execution":{"iopub.status.busy":"2023-09-28T18:38:52.290579Z","iopub.execute_input":"2023-09-28T18:38:52.291129Z","iopub.status.idle":"2023-09-28T18:38:57.276494Z","shell.execute_reply.started":"2023-09-28T18:38:52.291094Z","shell.execute_reply":"2023-09-28T18:38:57.275321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.show_model()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T18:39:24.128585Z","iopub.execute_input":"2023-09-28T18:39:24.129288Z","iopub.status.idle":"2023-09-28T18:39:24.201516Z","shell.execute_reply.started":"2023-09-28T18:39:24.129252Z","shell.execute_reply":"2023-09-28T18:39:24.200721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.train_generator(x_train, y_train, \n                                x_test, y_test, \n                                'UNet_model',\n                                epoch = 50,\n                                batch_size = BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T18:39:42.571674Z","iopub.execute_input":"2023-09-28T18:39:42.572075Z","iopub.status.idle":"2023-09-28T20:17:05.330112Z","shell.execute_reply.started":"2023-09-28T18:39:42.572045Z","shell.execute_reply":"2023-09-28T20:17:05.328950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}